# 深入MNNIST
- 在本教程中，我们将学到构建一个TensorFlow模型的基本步骤，并将通过这些步骤为MNIST构建一个深度卷积神经网络。

## 运行TensorFlow的InteractiveSession
- Tensorflow依赖于一个高效的C++后端来进行计算。与后端的这个连接叫做session。一般而言，使用TensorFlow程序的流程是先创建一个图，然后在session中启动它。

- 这里，我们使用更加方便的InteractiveSession类。通过它，你可以更加灵活地构建你的代码。它能让你在运行图的时候，插入一些计算图，这些计算图是由某些操作(operations)构成的。这对于工作在交互式环境中的人们来说非常便利，比如使用IPython。如果你没有使用InteractiveSession，那么你需要在启动session之前构建整个计算图，然后启动该计算图。

## 构建一个多层卷积网络
- 在MNIST上只有91%正确率，实在太糟糕。在这个小节里，我们用一个稍微复杂的模型：卷积神经网络来改善效果。这会达到大概99.2%的准确率。虽然不是最高，但是还是比较让人满意。

## 权重初始化
- 为了创建这个模型，我们需要创建大量的权重和偏置项。这个模型中的权重在初始化时应该加入少量的噪声来打破对称性以及避免0梯度。由于我们使用的是ReLU神经元，因此比较好的做法是用一个较小的正数来初始化偏置项，以避免神经元节点输出恒为0的问题（dead neurons）。为了不在建立模型的时候反复做初始化操作，我们定义两个函数用于初始化。

## [卷积和池化的定义](https://blog.csdn.net/weixin_42451919/article/details/81381294)

## RELU函数
- 在神经网络中，线性整流作为神经元的激活函数，定义了该神经元在线性变换  之后的非线性输出结果。换言之，对于进入神经元的来自上一层神经网络的输入向量  ，使用线性整流激活函数的神经元会输出

![函数公式](https://gss0.bdstatic.com/94o3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D117/sign=09b749e86c59252da7171905039a032c/2fdda3cc7cd98d10d1a7a7c32d3fb80e7aec90d5.jpg)

## dropout
参考[深度学习中Dropout原理解析](https://blog.csdn.net/program_developer/article/details/80737724)
- 过拟合：过拟合具体表现在：模型在训练数据上损失函数较小，预测准确率较高；但是在测试数据上损失函数比较大，预测准确率较低。
- 原理：在每个训练批次中，通过忽略一半的特征检测器（让一半的隐层节点值为0），可以明显地减少过拟合现象。这种方式可以减少特征检测器（隐层节点）间的相互作用，检测器相互作用是指某些检测器依赖其他检测器才能发挥作用。
